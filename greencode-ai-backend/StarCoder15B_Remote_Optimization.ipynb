{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GreenCode AI - Remote StarCoder 15B Optimization\n",
    "\n",
    "This notebook sets up the remote StarCoder 15B model for code optimization. It creates a simple API endpoint that can be used by the local backend when more advanced code optimization is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of Required Packages\n",
    "\n",
    "First, we need to install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.28.1 accelerate bitsandbytes flask flask-cors pyngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication with Hugging Face\n",
    "\n",
    "You'll need a Hugging Face account and token to access StarCoder. Create one at https://huggingface.co/ and generate a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load StarCoder 15B Model\n",
    "\n",
    "Now we'll load the full StarCoder 15B model. Colab has enough resources to handle this large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define model name\n",
    "model_name = \"bigcode/starcoder\"\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with low precision for efficiency\n",
    "print(\"Loading StarCoder 15B model... This may take a few minutes...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use half precision\n",
    "    device_map=\"auto\",          # Let the library handle device placement\n",
    "    load_in_8bit=True           # Use 8-bit quantization for more efficiency\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Flask API\n",
    "\n",
    "We'll create a simple Flask API that can receive code and return optimized versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for all routes\n",
    "\n",
    "def generate_optimized_code(code, context=\"energy_efficiency\"):\n",
    "    \"\"\"Use StarCoder 15B to generate optimized code\"\"\"\n",
    "    try:\n",
    "        # Different prompts based on optimization context\n",
    "        if context == \"energy_efficiency\":\n",
    "            prompt = f\"\"\"# Original Python code:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "# Task: Optimize the above code for maximum energy efficiency and sustainability.\n",
    "# Make it use less CPU, memory, and energy without changing functionality.\n",
    "\n",
    "# Optimized code for energy efficiency:\n",
    "```python\n",
    "\"\"\"\n",
    "        elif context == \"performance\":\n",
    "            prompt = f\"\"\"# Original Python code:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "# Task: Optimize the above code for maximum speed and performance.\n",
    "# Make it run faster without changing functionality.\n",
    "\n",
    "# Speed-optimized code:\n",
    "```python\n",
    "\"\"\"\n",
    "        elif context == \"memory_efficiency\":\n",
    "            prompt = f\"\"\"# Original Python code:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "# Task: Optimize the above code for minimum memory usage.\n",
    "# Make it use less RAM without changing functionality.\n",
    "\n",
    "# Memory-optimized code:\n",
    "```python\n",
    "\"\"\"\n",
    "        else:  # readability or default\n",
    "            prompt = f\"\"\"# Original Python code:\n",
    "```python\n",
    "{code}\n",
    "```\n",
    "\n",
    "# Task: Refactor the above code to be more readable and maintainable without changing functionality.\n",
    "# Use Python best practices and PEP 8 style guide.\n",
    "\n",
    "# Refactored code:\n",
    "```python\n",
    "\"\"\"\n",
    "        \n",
    "        # Generate with the model\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Generate optimized code\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=len(inputs.input_ids[0]) + 800,  # Allow for longer outputs\n",
    "                temperature=0.3,                           # Lower temperature for more focused output\n",
    "                top_p=0.95,                               # Nucleus sampling for some creativity\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode the result\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the optimized code part using regex\n",
    "        pattern = r\"```python\\n(.+?)(?:```|$)\"\n",
    "        matches = re.findall(pattern, generated_text, re.DOTALL)\n",
    "        \n",
    "        if len(matches) >= 2:  # We want the second code block (optimized version)\n",
    "            optimized_code = matches[1].strip()\n",
    "            return optimized_code\n",
    "        elif len(matches) == 1:  # Just in case there's only one block\n",
    "            optimized_code = matches[0].strip()\n",
    "            return optimized_code\n",
    "        else:\n",
    "            logger.error(\"Could not extract optimized code from model output\")\n",
    "            return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating optimized code: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "@app.route('/optimize', methods=['POST'])\n",
    "def optimize():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if not data or 'code' not in data:\n",
    "            return jsonify({'error': 'No code provided'}), 400\n",
    "        \n",
    "        code = data['code']\n",
    "        context = data.get('context', 'energy_efficiency')\n",
    "        \n",
    "        logger.info(f\"Received optimization request with context: {context}\")\n",
    "        \n",
    "        # Generate optimized code\n",
    "        optimized_code = generate_optimized_code(code, context)\n",
    "        \n",
    "        if optimized_code:\n",
    "            return jsonify({\n",
    "                'optimized_code': optimized_code,\n",
    "                'success': True\n",
    "            })\n",
    "        else:\n",
    "            return jsonify({\n",
    "                'error': 'Failed to generate optimized code',\n",
    "                'success': False\n",
    "            }), 500\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing request: {str(e)}\")\n",
    "        return jsonify({\n",
    "            'error': f'Error: {str(e)}',\n",
    "            'success': False\n",
    "        }), 500\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'model': 'StarCoder 15B',\n",
    "        'device': str(model.device)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Public URL with ngrok\n",
    "\n",
    "Now we'll use ngrok to create a public URL that can be accessed by the local backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# Start ngrok tunnel\n",
    "ngrok_tunnel = ngrok.connect(5000)\n",
    "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n",
    "print(\"\\nIMPORTANT: Copy this URL and set it as the COLAB_URL environment variable in your local backend!\")\n",
    "print(\"For example: export COLAB_URL=https://xxxx-xx-xxx-xxx-xx.ngrok.io/optimize\")\n",
    "\n",
    "# This is the URL you'll need to set in your local backend\n",
    "remote_api_url = f\"{ngrok_tunnel.public_url}/optimize\"\n",
    "print(f\"\\nFull API endpoint: {remote_api_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Flask Server\n",
    "\n",
    "Finally, we'll start the Flask server to handle API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Flask app\n",
    "print(\"Starting Flask server...\")\n",
    "app.run(host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Server\n",
    "\n",
    "You can test the server by sending a POST request to the API endpoint. Here's how you can test it with Python requests:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"<your_ngrok_url>/optimize\"\n",
    "data = {\n",
    "    \"code\": \"def calculate_values(data):\\n    result = []\\n    for item in data:\\n        if item > 0:\\n            result.append(item * 2)\\n    total = 0\\n    for r in result:\\n        total += r\\n    return result, total\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "This should return the optimized code from StarCoder 15B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. This notebook needs to be running for the remote API to work. Keep it open in your browser.\n",
    "2. Colab sessions have time limits. If your session disconnects, you'll need to restart it.\n",
    "3. The ngrok URL will change each time you restart the notebook.\n",
    "4. For a more permanent solution, consider deploying to a cloud provider."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}